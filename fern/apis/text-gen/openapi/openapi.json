{
  "components": {
    "schemas": {
      "ChatCompletionChoice": {
        "description": "A single chat completion choice. A response will contain one or\nmore of these based on the setting of `n`.",
        "properties": {
          "finish_reason": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FinishReason"
              },
              {
                "type": "null"
              }
            ]
          },
          "index": {
            "description": "A unique identifier for the chat completion.Each chunk has the same ID.",
            "title": "Index",
            "type": "integer"
          },
          "logprobs": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/Logprobs"
              },
              {
                "type": "null"
              }
            ]
          },
          "message": {
            "allOf": [
              {
                "$ref": "#/components/schemas/ChatMessage"
              }
            ],
            "description": "A chat completion message generated by the model."
          }
        },
        "required": [
          "index",
          "message"
        ],
        "title": "ChatCompletionChoice",
        "type": "object"
      },
      "ChatCompletionChunk": {
        "description": "Represents a single chunk of a streaming chat completion response.\n\nThis object's schema is compatible with OpenAI's Chat Completion API.",
        "properties": {
          "choices": {
            "description": "A list of chat completion choices.Can be more than one if n is greater than 1.",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionChunkChoice"
            },
            "title": "Choices",
            "type": "array"
          },
          "created": {
            "description": "The Unix timestamp (in seconds) of when the chat completion was created.",
            "examples": [
              1672342342
            ],
            "title": "Created",
            "type": "integer"
          },
          "id": {
            "description": "A unique identifier for the entire chat completion request. Each chunk in the stream has the same ID.",
            "title": "ID",
            "type": "string"
          },
          "model": {
            "description": "The model used for the chat completion.",
            "title": "Model",
            "type": "string"
          },
          "object": {
            "const": "chat.completion.chunk",
            "default": "chat.completion.chunk",
            "title": "Object"
          },
          "usage": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/UsageStats"
              },
              {
                "type": "null"
              }
            ]
          }
        },
        "required": [
          "id",
          "created",
          "model",
          "choices"
        ],
        "title": "ChatCompletionChunk",
        "type": "object"
      },
      "ChatCompletionChunkChoice": {
        "description": "An OpenAPI compatible schema for a chat completion chunk choice.",
        "properties": {
          "delta": {
            "$ref": "#/components/schemas/ChatCompletionDelta"
          },
          "finish_reason": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FinishReason"
              },
              {
                "type": "null"
              }
            ]
          },
          "index": {
            "title": "Index",
            "type": "integer"
          },
          "logprobs": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/Logprobs"
              },
              {
                "type": "null"
              }
            ]
          }
        },
        "required": [
          "index",
          "delta"
        ],
        "title": "ChatCompletionChunkChoice",
        "type": "object"
      },
      "ChatCompletionDelta": {
        "description": "An OpenAPI compatible schema for a chat completion choice delta.",
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Content"
          },
          "role": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Role"
          }
        },
        "required": [
          "role",
          "content"
        ],
        "title": "ChatCompletionDelta",
        "type": "object"
      },
      "ChatCompletionRequestExt": {
        "description": "OctoAI specific extensions for a chat completion request.",
        "properties": {
          "vllm": {
            "$ref": "#/components/schemas/ChatCompletionRequestExtVLLM"
          }
        },
        "title": "ChatCompletionRequestExt",
        "type": "object"
      },
      "ChatCompletionRequestExtVLLM": {
        "properties": {
          "best_of": {
            "default": 1,
            "title": "Best Of",
            "type": "integer"
          },
          "ignore_eos": {
            "default": false,
            "title": "Ignore Eos",
            "type": "boolean"
          },
          "skip_special_tokens": {
            "default": true,
            "title": "Skip Special Tokens",
            "type": "boolean"
          },
          "stop_token_ids": {
            "items": {
              "type": "integer"
            },
            "title": "Stop Token Ids",
            "type": "array"
          },
          "top_k": {
            "default": -1,
            "title": "Top K",
            "type": "integer"
          },
          "use_beam_search": {
            "default": false,
            "title": "Use Beam Search",
            "type": "boolean"
          }
        },
        "title": "ChatCompletionRequestExtVLLM",
        "type": "object"
      },
      "ChatCompletionResponse": {
        "description": "An OpenAI API compatible schema for a chat completion response object.",
        "properties": {
          "choices": {
            "description": "A list of chat completion choices.Can be more than one if n is greater than 1.",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionChoice"
            },
            "title": "Choices",
            "type": "array"
          },
          "created": {
            "description": "The Unix timestamp (in seconds) of when the chat completion was created.",
            "examples": [
              1672342342
            ],
            "title": "Created",
            "type": "integer"
          },
          "id": {
            "description": "A unique identifier for the chat completion.",
            "title": "Id",
            "type": "string"
          },
          "model": {
            "description": "The model used for the chat completion.",
            "title": "Model",
            "type": "string"
          },
          "object": {
            "const": "chat.completion",
            "default": "chat.completion",
            "title": "Object"
          },
          "usage": {
            "allOf": [
              {
                "$ref": "#/components/schemas/UsageStats"
              }
            ],
            "description": "Usage statistics for the completion request."
          }
        },
        "required": [
          "id",
          "created",
          "model",
          "choices",
          "usage"
        ],
        "title": "ChatCompletionResponse",
        "type": "object"
      },
      "ChatCompletionResponseFormat": {
        "properties": {
          "schema": {
            "anyOf": [
              {
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "title": "Schema"
          },
          "type": {
            "title": "Type",
            "type": "string"
          }
        },
        "required": [
          "type"
        ],
        "title": "ChatCompletionResponseFormat",
        "type": "object"
      },
      "ChatMessage": {
        "description": "An OpenAI API compatible schema for a single ChatMessage.",
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The contents of the message.",
            "title": "Content"
          },
          "role": {
            "description": "The role of the author of this message.",
            "title": "Role",
            "type": "string"
          }
        },
        "required": [
          "role",
          "content"
        ],
        "title": "ChatMessage",
        "type": "object"
      },
      "CompletionChoice": {
        "properties": {
          "finish_reason": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/FinishReason"
              },
              {
                "type": "null"
              }
            ]
          },
          "index": {
            "title": "Index",
            "type": "integer"
          },
          "logprobs": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/Logprobs"
              },
              {
                "type": "null"
              }
            ]
          },
          "text": {
            "title": "Text",
            "type": "string"
          }
        },
        "required": [
          "index",
          "text"
        ],
        "title": "CompletionChoice",
        "type": "object"
      },
      "CompletionResponse": {
        "description": "Represents a completion response from the API.\nNote: both the streamed and non-streamed response objects\nshare the same shape (unlike the chat endpoint).",
        "examples": [
          {
            "choices": [],
            "created": 11,
            "id": "cmpl-123",
            "model": "my_model",
            "system_fingerprint": "system_fingerprint"
          }
        ],
        "properties": {
          "choices": {
            "items": {
              "$ref": "#/components/schemas/CompletionChoice"
            },
            "title": "Choices",
            "type": "array"
          },
          "created": {
            "description": "The Unix timestamp (in seconds) of when the completion was created.",
            "examples": [
              1672342342
            ],
            "title": "Created",
            "type": "integer"
          },
          "id": {
            "title": "Id",
            "type": "string"
          },
          "model": {
            "title": "Model",
            "type": "string"
          },
          "object": {
            "const": "text_completion",
            "default": "text_completion",
            "title": "Object"
          },
          "system_fingerprint": {
            "title": "System Fingerprint",
            "type": "string"
          },
          "usage": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/UsageStats"
              },
              {
                "type": "null"
              }
            ]
          }
        },
        "required": [
          "id",
          "choices",
          "created",
          "model",
          "system_fingerprint"
        ],
        "title": "CompletionResponse",
        "type": "object"
      },
      "CreateChatCompletionRequest": {
        "description": "An OpenAI API compatible chat completion request.",
        "examples": [
          {
            "max_tokens": 128,
            "messages": [
              {
                "content": "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
                "role": "system"
              },
              {
                "content": "Hello world",
                "role": "user"
              }
            ],
            "model": "llama-2-13b-chat-fp16",
            "temperature": 0.1,
            "top_p": 0.9
          }
        ],
        "properties": {
          "frequency_penalty": {
            "default": 0.0,
            "description": "Penalizes new tokens based on their frequency in the generated text so far.",
            "maximum": 2.0,
            "minimum": -2.0,
            "title": "Frequency Penalty",
            "type": "number"
          },
          "ignore_eos": {
            "default": false,
            "title": "Ignore Eos",
            "type": "boolean"
          },
          "logit_bias": {
            "anyOf": [
              {
                "additionalProperties": {
                  "type": "number"
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "title": "Logit Bias"
          },
          "loglikelihood": {
            "default": false,
            "title": "Loglikelihood",
            "type": "boolean"
          },
          "logprobs": {
            "default": false,
            "title": "Logprobs",
            "type": "boolean"
          },
          "max_tokens": {
            "default": 512,
            "description": "Maximum number of tokens to generate per output sequence.",
            "minimum": 1.0,
            "title": "Max Tokens",
            "type": "integer"
          },
          "messages": {
            "description": "A list of messages comprising the conversation so far.",
            "items": {
              "$ref": "#/components/schemas/ChatMessage"
            },
            "title": "Messages",
            "type": "array"
          },
          "model": {
            "description": "The identifier of the model to use.Can be a shared tenancy or custom model identifier.",
            "title": "Model",
            "type": "string"
          },
          "n": {
            "default": 1,
            "minimum": 1.0,
            "title": "N",
            "type": "integer"
          },
          "octoai": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ChatCompletionRequestExt"
              },
              {
                "type": "null"
              }
            ]
          },
          "presence_penalty": {
            "default": 0.0,
            "description": "Penalizes new tokens based on whether they appear in the generated text so far",
            "maximum": 2.0,
            "minimum": -2.0,
            "title": "Presence Penalty",
            "type": "number"
          },
          "repetition_penalty": {
            "default": 1.0,
            "description": "Controls the likelihood of the model generating repeated texts",
            "exclusiveMinimum": 0.0,
            "title": "Repetition Penalty",
            "type": "number"
          },
          "response_format": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ChatCompletionResponseFormat"
              },
              {
                "type": "null"
              }
            ],
            "description": "Allows specification of a response format and associated schema that will constrain the LLM output to that structure. For example, using the `json_object` type allows you to provide a desired json schema for the output to follow."
          },
          "stop": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "type": "null"
              }
            ],
            "title": "Stop"
          },
          "stream": {
            "default": false,
            "description": "If set, tokens will be streamed incrementally to users.",
            "title": "Stream",
            "type": "boolean"
          },
          "stream_options": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/StreamOptions"
              },
              {
                "type": "null"
              }
            ],
            "description": "If set, usageStats will be streamed on the last content-containing chunk"
          },
          "temperature": {
            "default": 1.0,
            "description": "Controls the randomness of the sampling.",
            "maximum": 2.0,
            "minimum": 0.0,
            "title": "Temperature",
            "type": "number"
          },
          "top_logprobs": {
            "default": 0,
            "title": "Top Logprobs",
            "type": "integer"
          },
          "top_p": {
            "default": 1.0,
            "description": "Controls the cumulative probability of the top tokens to consider.",
            "exclusiveMinimum": 0.0,
            "maximum": 1.0,
            "title": "Top P",
            "type": "number"
          },
          "user": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "User"
          }
        },
        "required": [
          "model",
          "messages"
        ],
        "title": "CreateChatCompletionRequest",
        "type": "object"
      },
      "CreateCompletionRequest": {
        "description": "`POST https://text.octoai.run/v1/completions`\n\nCompletes the provided prefix prompt given the parameters.\n\nYou can view this as the \"raw\" API to the model which provides\nmore complete user control than the chat completions API.\n\nThis API is compatible with OpenAI's API.",
        "properties": {
          "best_of": {
            "default": 1,
            "description": "Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.",
            "title": "Best Of",
            "type": "integer"
          },
          "echo": {
            "default": false,
            "description": "Echo back the prompt in addition to the completion.",
            "title": "Echo",
            "type": "boolean"
          },
          "frequency_penalty": {
            "default": 0.0,
            "description": "Penalizes new tokens based on their frequency in the generated text so far.",
            "maximum": 2.0,
            "minimum": -2.0,
            "title": "Frequency Penalty",
            "type": "number"
          },
          "logit_bias": {
            "anyOf": [
              {
                "additionalProperties": {
                  "type": "number"
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "description": "Modify the likelihood of specified tokens appearing in the completion.",
            "title": "Logit Bias"
          },
          "loglikelihood": {
            "default": false,
            "description": "Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.",
            "title": "Loglikelihood",
            "type": "boolean"
          },
          "logprobs": {
            "anyOf": [
              {
                "maximum": 5.0,
                "minimum": 0.0,
                "type": "integer"
              },
              {
                "type": "null"
              }
            ],
            "description": "Number of log probabilities to return per output token.",
            "title": "Logprobs"
          },
          "max_tokens": {
            "default": 16,
            "description": "Maximum number of tokens to generate per output sequence.",
            "minimum": 1.0,
            "title": "Max Tokens",
            "type": "integer"
          },
          "model": {
            "description": "Model name to use for completion.",
            "title": "Model",
            "type": "string"
          },
          "n": {
            "default": 1,
            "description": "Number of output sequences to return.",
            "minimum": 1.0,
            "title": "N",
            "type": "integer"
          },
          "presence_penalty": {
            "default": 0.0,
            "description": "Penalizes new tokens based on whether they appear in the generated text so far",
            "maximum": 2.0,
            "minimum": -2.0,
            "title": "Presence Penalty",
            "type": "number"
          },
          "prompt": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "items": {
                  "type": "integer"
                },
                "type": "array"
              },
              {
                "items": {
                  "items": {
                    "type": "integer"
                  },
                  "type": "array"
                },
                "type": "array"
              },
              {
                "type": "null"
              }
            ],
            "description": "The prompt to generate completions from.",
            "title": "Prompt"
          },
          "repetition_penalty": {
            "default": 1.0,
            "description": "Controls the likelihood of the model generating repeated texts",
            "exclusiveMinimum": 0.0,
            "title": "Repetition Penalty",
            "type": "number"
          },
          "seed": {
            "default": 0,
            "title": "Seed",
            "type": "integer"
          },
          "stop": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "type": "null"
              }
            ],
            "description": "Strings that stop the generation when they are generated.",
            "title": "Stop"
          },
          "stream": {
            "default": false,
            "description": "If set, tokens will be streamed incrementally to users.",
            "title": "Stream",
            "type": "boolean"
          },
          "stream_options": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/StreamOptions"
              },
              {
                "type": "null"
              }
            ],
            "description": "If set, usageStats will be streamed on the last content-containing chunk"
          },
          "suffix": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "description": "The suffix that comes after a completion of inserted text.",
            "title": "Suffix"
          },
          "temperature": {
            "default": 1.0,
            "description": "Controls the randomness of the sampling.",
            "maximum": 2.0,
            "minimum": 0.0,
            "title": "Temperature",
            "type": "number"
          },
          "top_p": {
            "default": 1.0,
            "description": "Controls the cumulative probability of the top tokens to consider.",
            "exclusiveMinimum": 0.0,
            "maximum": 1.0,
            "title": "Top P",
            "type": "number"
          },
          "user": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "User"
          }
        },
        "required": [
          "model",
          "prompt"
        ],
        "title": "CreateCompletionRequest",
        "type": "object"
      },
      "ErrorResponse": {
        "description": "An OpenAI API compatible schema for a error response.",
        "properties": {
          "code": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Code"
          },
          "message": {
            "title": "Message",
            "type": "string"
          },
          "object": {
            "default": "error",
            "title": "Object",
            "type": "string"
          },
          "param": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "null"
              }
            ],
            "title": "Param"
          },
          "type": {
            "title": "Type",
            "type": "string"
          },
          "validation_errors": {
            "anyOf": [
              {
                "additionalProperties": {
                  "anyOf": [
                    {
                      "type": "string"
                    },
                    {
                      "items": {
                        "type": "string"
                      },
                      "type": "array"
                    }
                  ]
                },
                "type": "object"
              },
              {
                "type": "null"
              }
            ],
            "title": "Validation Errors"
          }
        },
        "required": [
          "message",
          "type"
        ],
        "title": "ErrorResponse",
        "type": "object"
      },
      "FinishReason": {
        "description": "The reason why the model stopped generating tokens.\n\nThis will be `stop` if the model naturally completed generation or encountered a\nprovided stop sequence, `length` if the maximum number of tokens specified in the\nrequest was reached, content_filter if content was omitted due to a flag from our\ncontent filters, tool_calls if the model called a tool, or function_call\n(deprecated) if the model called a function.",
        "enum": [
          "stop",
          "length",
          "tool_calls",
          "content_filter",
          "function_call",
          "cancelled"
        ],
        "title": "FinishReason",
        "type": "string"
      },
      "FunctionCall": {
        "description": "The representation of a function called during tool use.",
        "properties": {
          "arguments": {
            "description": "The arguments to the function call.",
            "title": "Arguments",
            "type": "string"
          },
          "name": {
            "description": "The name of the function to call.",
            "title": "Name",
            "type": "string"
          }
        },
        "required": [
          "name",
          "arguments"
        ],
        "title": "FunctionCall",
        "type": "object"
      },
      "HTTPValidationError": {
        "properties": {
          "detail": {
            "items": {
              "$ref": "#/components/schemas/ValidationError"
            },
            "title": "Detail",
            "type": "array"
          }
        },
        "title": "HTTPValidationError",
        "type": "object"
      },
      "Logprobs": {
        "description": "An OpenAI API compatible schema for logprobs output.\nSee details in https://platform.openai.com/docs/api-reference/chat/object#chat-create-logprobs",
        "properties": {
          "content": {
            "description": "List of logprobs info",
            "items": {
              "anyOf": [
                {
                  "$ref": "#/components/schemas/LogprobsContent"
                },
                {
                  "type": "null"
                }
              ]
            },
            "title": "Content",
            "type": "array"
          }
        },
        "title": "Logprobs",
        "type": "object"
      },
      "LogprobsContent": {
        "description": "An OpenAI API compatible schema for logprobs output.",
        "properties": {
          "bytes": {
            "anyOf": [
              {
                "items": {},
                "type": "array"
              },
              {
                "type": "null"
              }
            ],
            "description": "A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.",
            "title": "Bytes"
          },
          "logprob": {
            "description": "Logprob corresponding to the token",
            "title": "Logprob",
            "type": "number"
          },
          "token": {
            "description": "New generated token or token from prompt for loglikelihood case",
            "title": "Token",
            "type": "string"
          },
          "top_logprobs": {
            "description": "List of top tokens info",
            "items": {
              "$ref": "#/components/schemas/TopLogprobs"
            },
            "title": "Top Logprobs",
            "type": "array"
          }
        },
        "required": [
          "token",
          "logprob"
        ],
        "title": "LogprobsContent",
        "type": "object"
      },
      "StreamOptions": {
        "properties": {
          "include_usage": {
            "default": false,
            "description": "Whether or not to include token usage stats on the final chunk before the [Done] message.",
            "title": "Include Usage",
            "type": "boolean"
          }
        },
        "title": "StreamOptions",
        "type": "object"
      },
      "ToolCall": {
        "description": "An OpenAI API compatible schema for a tool invocation.",
        "properties": {
          "function": {
            "allOf": [
              {
                "$ref": "#/components/schemas/FunctionCall"
              }
            ],
            "description": "The function called by the model."
          },
          "id": {
            "description": "A unique ID for the tool call used to reference it in the response.",
            "title": "Id",
            "type": "string"
          },
          "type": {
            "const": "function",
            "default": "function",
            "description": "The type of tool. Today only `function` is supported.",
            "title": "Type"
          }
        },
        "required": [
          "id",
          "function"
        ],
        "title": "ToolCall",
        "type": "object"
      },
      "TopLogprobs": {
        "description": "An OpenAI API compatible schema for logprobs output.",
        "properties": {
          "bytes": {
            "anyOf": [
              {
                "items": {},
                "type": "array"
              },
              {
                "type": "null"
              }
            ],
            "description": "A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.",
            "title": "Bytes"
          },
          "logprob": {
            "description": "Logprob corresponding to the top token",
            "title": "Logprob",
            "type": "number"
          },
          "token": {
            "description": "Token from the top list",
            "title": "Token",
            "type": "string"
          }
        },
        "required": [
          "token",
          "logprob"
        ],
        "title": "TopLogprobs",
        "type": "object"
      },
      "UsageStats": {
        "description": "The token usage statistics for a request.",
        "properties": {
          "completion_tokens": {
            "description": "Number of tokens in the prompt.",
            "title": "Completion Tokens",
            "type": "integer"
          },
          "prompt_tokens": {
            "description": "Number of tokens in the prompt.",
            "title": "Prompt Tokens",
            "type": "integer"
          },
          "total_tokens": {
            "description": "Total number of tokens used in the request (prompt + completion).",
            "title": "Total Tokens",
            "type": "integer"
          }
        },
        "required": [
          "prompt_tokens",
          "completion_tokens",
          "total_tokens"
        ],
        "title": "UsageStats",
        "type": "object"
      },
      "ValidationError": {
        "properties": {
          "loc": {
            "items": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "integer"
                }
              ]
            },
            "title": "Location",
            "type": "array"
          },
          "msg": {
            "title": "Message",
            "type": "string"
          },
          "type": {
            "title": "Error Type",
            "type": "string"
          }
        },
        "required": [
          "loc",
          "msg",
          "type"
        ],
        "title": "ValidationError",
        "type": "object"
      }
    }
  },
  "info": {
    "title": "ollm-api",
    "version": "0.1"
  },
  "openapi": "3.1.0",
  "paths": {
    "/v1/chat/completions": {
      "post": {
        "description": "Create a Chat Completion.",
        "operationId": "create_chat_completion_v1_chat_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateChatCompletionRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "content": {
              "application/json": {
                "examples": [],
                "schema": {
                  "$ref": "#/components/schemas/ChatCompletionResponse",
                  "anyOf": [
                    {
                      "$ref": "#/components/schemas/ChatCompletionResponse"
                    },
                    {
                      "$ref": "#/components/schemas/ChatCompletionChunk"
                    }
                  ],
                  "title": "Response 200 Create Chat Completion V1 Chat Completions Post"
                }
              },
              "text/event-stream": {
                "examples": [],
                "schema": {
                  "$ref": "#/components/schemas/ChatCompletionChunk"
                }
              }
            },
            "description": "An OctoAI text endpoint can be requested in either a synchronous orstreaming mode.When the request body has `stream: False` set the content type will be`application/json`. When the request body has `stream: True` set thecontent type will be `text-event-stream` and will respond with a stream of[server-send-events (SSE)](https://en.wikipedia.org/wiki/Server-sent_events)."
          },
          "422": {
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            },
            "description": "Validation Error"
          },
          "500": {
            "content": {
              "application/json": {
                "example": {
                  "message": "An example error",
                  "object": "error",
                  "type": "internal_error"
                },
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                }
              }
            },
            "description": "Internal Server Error"
          }
        },
        "summary": "Create Chat Completion",
        "tags": [
          "text"
        ],
        "x-fern-streaming": {
          "response": {
            "$ref": "#/components/schemas/ChatCompletionResponse"
          },
          "response-stream": {
            "$ref": "#/components/schemas/ChatCompletionChunk"
          },
          "stream-condition": "$request.stream"
        }
      }
    },
    "/v1/completions": {
      "post": {
        "operationId": "create_completion_v1_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateCompletionRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "content": {
              "application/json": {
                "examples": [
                  {
                    "choices": [],
                    "created": 11,
                    "id": "cmpl-123",
                    "model": "my_model",
                    "system_fingerprint": "system_fingerprint"
                  }
                ],
                "schema": {
                  "$ref": "#/components/schemas/CompletionResponse"
                }
              },
              "text/event-stream": {
                "examples": [
                  {
                    "choices": [],
                    "created": 11,
                    "id": "cmpl-123",
                    "model": "my_model",
                    "system_fingerprint": "system_fingerprint"
                  }
                ],
                "schema": {
                  "$ref": "#/components/schemas/CompletionResponse"
                }
              }
            },
            "description": "An OctoAI text endpoint can be requested in either a synchronous orstreaming mode.When the request body has `stream: False` set the content type will be`application/json`. When the request body has `stream: True` set thecontent type will be `text-event-stream` and will respond with a stream of[server-send-events (SSE)](https://en.wikipedia.org/wiki/Server-sent_events)."
          },
          "422": {
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            },
            "description": "Validation Error"
          },
          "500": {
            "content": {
              "application/json": {
                "example": {
                  "message": "An example error",
                  "object": "error",
                  "type": "internal_error"
                },
                "schema": {
                  "$ref": "#/components/schemas/ErrorResponse"
                }
              }
            },
            "description": "Internal Server Error"
          }
        },
        "summary": "Create Completion",
        "tags": [
          "text"
        ],
        "x-fern-streaming": {
          "response": {
            "$ref": "#/components/schemas/CompletionResponse"
          },
          "response-stream": {
            "$ref": "#/components/schemas/CompletionResponse"
          },
          "stream-condition": "$request.stream"
        }
      }
    }
  }
}
